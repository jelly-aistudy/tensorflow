import numpy as np
import random

import tensorflow as tf
import matplotlib.pyplot as plt
from tqdm import tqdm
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM 

# Naver Movie Review Data
!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt
!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt

with open("ratings_train.txt") as f:
    train_raw = f.readlines()
with open("ratings_test.txt") as f:
    test_raw = f.readlines()
train_raw = [t.split('\t') for t in train_raw[1:]]
test_raw = [t.split('\t') for t in test_raw[1:]]

train_all = []
test_all = []
for line in train_raw:
    train_all.append([line[0], line[1], int(line[2].strip())])
for line in test_raw:
    test_all.append([line[0], line[1], int(line[2].strip())]) 

# Use partial data
random.seed(1)
random.shuffle(train_all)
random.shuffle(test_all)
train = train_all[:50000]
val = train_all[50000:60000]
test = test_all[:10000]

# Tokenize
train_sentences = []
val_sentences = []
test_sentences = []
train_label_ids = []
val_label_ids = []
test_label_ids = []

def tokenize(sentence): 
  return [char for char in sentence]

for i, data in enumerate(train):
    words = tokenize(data[1])
    train_sentences.append(words)
    if data[2] == 0: # Negative
      train_label_ids.append([1,0])
    else: # Positive
      train_label_ids.append([0,1])

for data in val:
    words = tokenize(data[1])
    val_sentences.append(words)
    if data[2] == 0: # Negative
      val_label_ids.append([1,0])
    else: # Positive
      val_label_ids.append([0,1])

for data in test:
    words = tokenize(data[1])
    test_sentences.append(words)
    if line[2] == 0: # Negative
      test_label_ids.append([1,0])
    else: # Positive
      test_label_ids.append([0,1])


# Vocabulary Dictionary
vocab_dict = {}
vocab_dict["[PAD]"] = 0
vocab_dict["[OOV]"] = 1
i = 2
for sentence in train_sentences:
    for word in sentence:
        if word not in vocab_dict.keys(): 
            vocab_dict[word] = i
            i += 1

# Character to Index
def make_input_ids(tokenized_sentences, max_seq_len = 50):
  
  num_oov = 0 
  result_input_ids = [] 

  for sentence in tokenized_sentences :
      input_ids = []
      for word in sentence:
          if word not in vocab_dict:  
              input_ids.append(vocab_dict['[OOV]']) 
              num_oov += 1
          else:                    
              input_ids.append(vocab_dict[word]) 
      result_input_ids.append(input_ids)
  
  # padding
  result_input_ids = pad_sequences(result_input_ids, maxlen=max_seq_len, value=vocab_dict["[PAD]"])

  return result_input_ids, num_oov


train_input_ids, train_num_oov = make_input_ids(train_sentences)
val_input_ids, val_num_oov = make_input_ids(val_sentences)
test_input_ids, test_num_oov = make_input_ids(test_sentences)
train_label_ids = np.array(train_label_ids)
val_label_ids = np.array(val_label_ids)
test_label_ids = np.array(test_label_ids)

# Model
vocab_size = len(vocab_dict) 
model = Sequential([
            Embedding(vocab_size, 150),
            LSTM(100, activation='relu'),
            Dense(100, activation='relu'),
            Dense(2, activation='softmax')
])  

model.summary()

EPOCHS = 10
BATCHS = 256

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(train_input_ids, train_label_ids, validation_data=(val_input_ids, val_label_ids), epochs=EPOCHS, batch_size=BATCHS, verbose=1) 

# Accuracy and Loss
def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

# Inference
def inference(mymodel, sentence):
  words = tokenize(sentence)
  input_id = []
  for word in words:
    if word in vocab_dict: input_id.append(vocab_dict[word])
    else: input_id.append(vocab_dict["[OOV]"])
  
  score = mymodel.predict(np.array([input_id])) 

  print(" input sentence:", sentence)
  print("   -> Negative : {:.2f} / Positive : {:.2f}".format(score[0][0],score[0][1]))

# Example
sentence1 = "원래 안 보려고 했는데 보다 보니까 시간 가는 줄 모르고 봤음 추천"
sentence2 = "이거 볼 시간에 헬스장 가서 운동하는 게 삶에 도움이 됩니다"
inference(model, sentence1)
inference(model, sentence2)
