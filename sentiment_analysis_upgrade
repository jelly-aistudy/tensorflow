import pickle
import numpy as np
import json
import tensorflow as tf
import random

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Bidirectional, Dropout
from utils import TextEncoder
from google.colab import drive
drive.mount('/content/gdrive')

!cp "/content/gdrive/My Drive/NLP/utils.py" "/content"

# load preprocessed data
with open("/content/gdrive/My Drive/NLP/Sentiment_preprocess_data.pkl", "rb") as f:
  preprocess_data = pickle.load(f)
train_ids = preprocess_data["train_ids"]
train_labels = preprocess_data["train_labels"]
val_ids = preprocess_data["val_ids"]
val_labels = preprocess_data["val_labels"]
test_ids = preprocess_data["test_ids"]
test_labels = preprocess_data["test_labels"]
label_map = preprocess_data["label_map"]

# load vocab list
with open("/content/gdrive/My Drive/NLP/Sentiment_vocab.json", "r") as f:
  new_vocab_list = json.loads(f.read())
# load embeddings
with open("/content/gdrive/My Drive/NLP/embeddings.tsv") as f:
  vecs = [v.strip() for v in f.readlines()]
  final_embeddings = [v.split("\t") for v in vecs]
  final_embeddings = np.array(final_embeddings, dtype="float32")

text_encoder = TextEncoder(new_vocab_list)

tf.keras.backend.clear_session()

############ LSTM ############
vocab_size = text_encoder.vocab_size 
embedding_dim = final_embeddings.shape[1] 
rnn_hidden_dim = 50 
final_dim = len(label_map)

model1 = Sequential(
    [Embedding(vocab_size, embedding_dim, mask_zero=True),
     LSTM(rnn_hidden_dim),
     Dense(rnn_hidden_dim, activation= "relu"),
     Dense(2, activation="softmax")]
)

org_vocab_size = final_embeddings.shape[0]
rand_initial = np.random.uniform(-1,1,size=[vocab_size-org_vocab_size,embedding_dim])
initial_weight = np.append(final_embeddings, rand_initial, axis = 0)
model1.weights[0].assign(initial_weight)
model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=1)
num_epochs = 5
history = model1.fit(train_ids, train_labels, epochs=num_epochs, batch_size=200,
                    validation_data=(val_ids, val_labels), callbacks=[callback])
model1.evaluate(test_ids, test_labels)


############ Bi-LSTM ############
model2 = Sequential(
    [Embedding(vocab_size, embedding_dim, mask_zero=True),
     Bidirectional(LSTM(rnn_hidden_dim)),
     Dense(rnn_hidden_dim, activation= "relu"),
     Dense(2, activation="softmax")]
)
model2.weights[0].assign(initial_weight)
model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=1)
num_epochs = 5
history = model2.fit(train_ids, train_labels, epochs=num_epochs, batch_size=200,
                    validation_data=(val_ids, val_labels), callbacks=[callback])
model2.evaluate(test_ids, test_labels)


############ Multi-layer-LSTM ############
model3 = Sequential(
    [Embedding(vocab_size, embedding_dim, mask_zero=True),
     GRU(rnn_hidden_dim, return_sequences = True),
     Dropout(0.2),
     LSTM(rnn_hidden_dim, return_sequences = False),
     Dense(2, activation="softmax")]
)
model3.weights[0].assign(initial_weight)
model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=1)
num_epochs = 5
history = model3.fit(train_ids, train_labels, epochs=num_epochs, batch_size=200,
                    validation_data=(val_ids, val_labels), callbacks=[callback])
model3.evaluate(test_ids, test_labels)


############ Ensemble ############
def predict(test_ids):
  res1 = model1.predict(test_ids)
  res2 = model2.predict(test_ids)
  res3 = model3.predict(test_ids)
  result = (res1 + res2 + res3) / 3
  return result
  
prediction = predict(test_ids)
prediction
prediction = np.argmax(prediction, axis = 1) # change into category
print("TEST ACCURACY: ", sum(prediction == test_labels) / len(test_labels))
