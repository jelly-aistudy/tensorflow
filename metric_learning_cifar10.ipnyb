import random
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from collections import defaultdict
from PIL import Image
from sklearn.metrics import ConfusionMatrixDisplay
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

x_train = x_train.astype("float32") / 255.0    # normalize
# y_train[0] = [6]
y_train = np.squeeze(y_train) # squeeze: Remove axes of length one from a. see example here -> https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html
x_test = x_test.astype("float32") / 255.0
y_test = np.squeeze(y_test)

height_width = 32

## sample data image show ##
def show_collage(examples):
    box_size = height_width + 2
    num_rows, num_cols = examples.shape[:2]

    collage = Image.new(
        mode="RGB",
        size=(num_cols * box_size, num_rows * box_size),
        color=(250, 250, 250),
    )
    for row_idx in range(num_rows):
        for col_idx in range(num_cols):
            array = (np.array(examples[row_idx, col_idx]) * 255).astype(np.uint8)
            collage.paste(
                Image.fromarray(array), (col_idx * box_size, row_idx * box_size)
            )

    collage = collage.resize((2 * num_cols * box_size, 2 * num_rows * box_size))
    return collage
    
sample_idxs = np.random.randint(0, 50000, size=(5, 5))
examples = x_train[sample_idxs]
show_collage(examples)

class_idx_to_train_idxs = defaultdict(list)
# The functionality of both dictionaries and defualtdict are almost same except for the fact that defualtdict never raises a KeyError. It provides a default value for the key that does not exists.
for y_train_idx, y in enumerate(y_train):
    class_idx_to_train_idxs[y].append(y_train_idx)

class_idx_to_test_idxs = defaultdict(list)
for y_test_idx, y in enumerate(y_test):
    class_idx_to_test_idxs[y].append(y_test_idx)

num_classes = 10    # number of cifar10 class

class AnchorPositivePairs(keras.utils.Sequence):
    def __init__(self, num_batchs):
        self.num_batchs = num_batchs

    def __len__(self):
        return self.num_batchs

    def __getitem__(self, _idx):
        # numpy.empty : Return a new array of given shape and type, without initializing entries.
        x = np.empty((2, num_classes, height_width, height_width, 3), dtype=np.float32)  # First dimension is 2 in order to make anchor-positive pair
        for class_idx in range(num_classes):
            examples_for_class = class_idx_to_train_idxs[class_idx]  # Set of sample index for specific class index 
            anchor_idx = random.choice(examples_for_class)  # Randomly choose index from sample index set for anchor
            positive_idx = random.choice(examples_for_class)  # Randomly choose index from sample index set for positive
            while positive_idx == anchor_idx:  # anchor and positive must not be the same. if so, choose another positive
                positive_idx = random.choice(examples_for_class)
            x[0, class_idx] = x_train[anchor_idx]  
            x[1, class_idx] = x_train[positive_idx]  
        return x
        
examples = next(iter(AnchorPositivePairs(num_batchs=1)))
show_collage(examples)
# first row - anchor image, second row - positive image

class EmbeddingModel(keras.Model):
    def train_step(self, data):
        if isinstance(data, tuple): # The isinstance() function checks if the object (first argument) is an instance or subclass of classinfo class (second argument).
            data = data[0]
        anchors, positives = data[0], data[1]   

        with tf.GradientTape() as tape: # see more explanations here : https://www.tensorflow.org/api_docs/python/tf/GradientTape
            # Compute embedding vector 
            anchor_embeddings = self(anchors, training=True)
            positive_embeddings = self(positives, training=True)

            # Compute cosine similarity between anchor and positive
            # see more explanations here : https://www.tensorflow.org/api_docs/python/tf/einsum
            similarities = tf.einsum(
                "ae,pe->ap", anchor_embeddings, positive_embeddings
            )

            # We use similarity for logits, so adjust it for train efficiency
            similarities /= 0.2

            # labels corresponding to logits. Here, label = class index
            sparse_labels = tf.range(num_classes)
            loss = self.compiled_loss(sparse_labels, similarities)

        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        self.compiled_metrics.update_state(sparse_labels, similarities)
        return {m.name: m.result() for m in self.metrics}
        
inputs = layers.Input(shape=(height_width, height_width, 3))
x = layers.Conv2D(32, 3, activation="relu")(inputs)
x = layers.Conv2D(64, 3, activation="relu")(x)
x = layers.Conv2D(128, 3, activation="relu")(x)
x = layers.GlobalAveragePooling2D()(x)
embeddings = layers.Dense(units=8, activation=None)(x)
embeddings = tf.nn.l2_normalize(embeddings, axis=-1) # Normalizes along dimension axis using an L2 norm.

model = EmbeddingModel(inputs, embeddings)

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
)

history = model.fit(AnchorPositivePairs(num_batchs=1000), epochs=40)

plt.plot(history.history["loss"])
plt.show()

train_embed = model.predict(x_train)
test_embed = model.predict(x_test)
sim_mat = tf.einsum("ae,be->ab", test_embed, train_embed) 
top1_test = np.argsort(sim_mat)[:,-2:]  # return two index which have biggest similarity in train dataset 

cnt = 0
for row_idx in range(len(y_test)):
    top1_idx = top1_test[row_idx][1]   # first index has the biggest similarity in train dataset
    top1_class = y_train[top1_idx]   # class of that train data 
    if y_test[row_idx] == top1_class:
        cnt+=1

# cnt : number of test samples which have the most similar image

labels = [
    "Airplane",
    "Automobile",
    "Bird",
    "Cat",
    "Deer",
    "Dog",
    "Frog",
    "Horse",
    "Ship",
    "Truck",
]

rows = 10
axes=[]
fig=plt.figure(figsize=(4,14))

for n in range(rows):
    axes.append( fig.add_subplot(rows, 2, 2*n+1) )
    subplot_title=(str(labels[y_test[n]]))
    axes[-1].set_title(subplot_title)  
    plt.imshow(x_test[n])
    plt.axis('off')

    top1_idx = top1_test[n][1]
    axes.append( fig.add_subplot(rows, 2, 2*n+2) )
    subplot_title=(str(labels[y_train[top1_idx]]))
    color = "green" if labels[y_train[top1_idx]] == labels[y_test[n]] else "red"
    axes[-1].set_title(subplot_title, color=color)     
    plt.imshow(x_train[top1_idx])
    plt.axis('off')
fig.tight_layout()    
plt.show()
