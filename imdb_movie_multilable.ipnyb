!wget -O utils.py https://raw.githubusercontent.com/ashrefm/multi-label-soft-f1/master/utils.py
!wget -O movie.csv https://raw.githubusercontent.com/ashrefm/multi-label-soft-f1/master/data/movie_poster/MovieGenre.csv

from utils import *
from PIL import Image
from sklearn.preprocessing import MultiLabelBinarizer
import matplotlib.pyplot as plt
import os
import pandas as pd 
import tensorflow as tf
import tensorflow_hub as hub 
from keras.preprocessing import image 
from tensorflow.keras.layers import Dense, Dropout

movies = pd.read_csv("./movie.csv", encoding="ISO-8859-1")
movies.dropna(subset=['imdbId', 'Genre', 'Poster'], inplace=True)  # If inplace=True, do operation inplace and return None
movies = movies[:10000]
## movies.head(3) # check movie data

download_dir = './data/movie_poster/images' 
movies = download_parallel(movies, download_dir)  # Downloads images from Internet in parallel (utils.py)

# use genres in which at least 1000 movies are included
label_freq = movies['Genre'].apply(lambda s: str(s).split('|')).explode().value_counts().sort_values(ascending=False)
least_used_label = list(label_freq[label_freq<1000].index)
movies['Genre'] = movies['Genre'].apply(lambda s: [l for l in str(s).split('|') if l not in least_used_label]) 

# explode : see examples here -> https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html
## label_freq.head(3) # check label_freq data 
# Drama      5011
# Comedy     3411
# Romance    1859
# Name: Genre, dtype: int64
## for i in range(3): # check least_used_label data
##  print(least_used_label[i])
# Horror
# Mystery
# Fantasy
## movies.head(3) # check movie data

x_train, x_val = movies['imdbId'][:8000], movies['imdbId'][8000:]
y_train, y_val = movies['Genre'][:8000], movies['Genre'][8000:]
x_train = [os.path.join('./data/movie_poster/images', str(f)+'.jpg') for f in x_train]
x_val = [os.path.join('./data/movie_poster/images', str(f)+'.jpg') for f in x_val] 
y_train = list(y_train)
y_val = list(y_val) 

# see image
style.use("default")
plt.figure(figsize=(12,4))
for i in range(4):
    ax = plt.subplot(1, 4, i+1)
    plt.imshow(Image.open(x_train[i]))
    plt.title(y_train[i], size=10)
    plt.axis('off')
    
# multi labels
mlb = MultiLabelBinarizer()
mlb.fit(y_train)
LABEL_NUM = len(mlb.classes_)

# check all multi class labels
# for (i, label) in enumerate(mlb.classes_):
#     print("{}. {}".format(i, label))

# label string to vector
y_train_trans = mlb.transform(y_train)
y_val_trans = mlb.transform(y_val)

IMG_SIZE = 224  
CHANNELS = 3 
BATCH_SIZE = 256  
LR = 0.0001 
EPOCHS = 50

# normalize data
def normalize(filename, label): 
    image_string = tf.io.read_file(filename) 
    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS) # Decode a JPEG-encoded image to a uint8 tensor. The attr channels indicates the desired number of color channels for the decoded image.
    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE]) 
    image_normalized = image_resized / 255.0
    return image_normalized, label

# create dataset
def create_dataset(filenames, labels):
    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels)) 
    dataset = dataset.map(normalize) 
    dataset = dataset.batch(BATCH_SIZE)  
    return dataset

train_set = create_dataset(x_train, y_train_trans)
val_set = create_dataset(x_val, y_val_trans)

feature_extractor_url = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2" 

model = tf.keras.Sequential([
    hub.KerasLayer(feature_extractor_url, input_shape=(IMG_SIZE,IMG_SIZE,CHANNELS)),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(LABEL_NUM, activation='sigmoid') # don't use softmax, we need multi label
])

model.summary()

earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR), loss='mse')
history = model.fit(train_set, epochs=EPOCHS, validation_data=val_set, callbacks=[earlystop])

# see loss graph
loss = history.history['loss']
val_loss = history.history['val_loss'] 
epochs = len(loss) 
plt.figure(figsize=(8, 4))

plt.plot(range(1, epochs+1), loss, label='Training Loss')
plt.plot(range(1, epochs+1), val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.show() 

# inference
def inference(img_path, model): 
    # read and prepare image
    img = image.load_img(img_path, target_size=(IMG_SIZE,IMG_SIZE,CHANNELS))
    img = image.img_to_array(img)
    img = img/255
    img = np.expand_dims(img, axis=0) # expand_dims: see examples here -> https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html

    # Generate prediction
    prediction = (model.predict(img) > 0.5).astype('int')
    # print(prediction) # example: [[0 0 1 0 0 1 0]]
    prediction = pd.Series(prediction[0])
    # print(prediction)
    # example:
    # 0    0
    # 1    0
    # 2    1
    # 3    0
    # 4    0
    # 5    1
    # 6    0
    # dtype: int64
    prediction.index = mlb.classes_
    prediction = prediction[prediction==1].index.values 
    if len(prediction)==0:
      prediction = [mlb.classes_[tf.argmax(model.predict(img)[0])]]

    # show image with prediction
    style.use('default')
    plt.figure(figsize=(8,4))
    plt.imshow(Image.open(img_path))
    plt.title('Prediction\n{}\n'.format(list(prediction)), fontsize=12)
    plt.show()

# test real data
# Netflix Move To Heaven poster
!wget -O test.jpg https://thumb.mtstarnews.com/06/2021/05/2021050210564645380_1.jpg/dims/optimize
inference("./test.jpg", model)
