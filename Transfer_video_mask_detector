import dlib
import cv2
import matplotlib.pyplot as plt
import copy
import os
import numpy as np

# download face detector model
!wget http://dlib.net/files/mmod_human_face_detector.dat.bz2 -O mmod_human_face_detector.dat.bz2
!bzip2 -kd mmod_human_face_detector.dat.bz2

cnn_face_detector = dlib.cnn_face_detection_model_v1('mmod_human_face_detector.dat')

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator

#### DATA ####
!git clone https://github.com/narfian/SyntheticMask_Dataset

PATH = os.path.join('/content/SyntheticMask_Dataset/data')
MODEL_PATH = './models'

train_dir = os.path.join(PATH, 'train')
valid_dir = os.path.join(PATH, 'val')
test_dir = os.path.join(PATH, 'test')

train_wm_dir = os.path.join(train_dir, 'with_mask')
train_nm_dir = os.path.join(train_dir, 'without_mask')
valid_wm_dir = os.path.join(valid_dir, 'with_mask')
valid_nm_dir = os.path.join(valid_dir, 'without_mask')
test_wm_dir = os.path.join(test_dir, 'with_mask')
test_nm_dir = os.path.join(test_dir, 'without_mask')

num_wm_tr = len(os.listdir(train_wm_dir))
num_nm_tr = len(os.listdir(train_nm_dir))
num_wm_val = len(os.listdir(valid_wm_dir))
num_nm_val = len(os.listdir(valid_nm_dir))
num_wm_te = len(os.listdir(test_wm_dir))
num_nm_te = len(os.listdir(test_nm_dir))

total_train = num_wm_tr + num_nm_tr
total_valid = num_wm_val + num_nm_val
total_test = num_wm_te + num_nm_te

print('train with mask : {0}, train without mask : {1}, validation with mask : {2}, validation without mask: {3}, test with mask: {4}, test without mask: {5}'.format(num_wm_tr, num_nm_tr, num_wm_val, num_nm_val, num_wm_te, num_nm_te))

BATCH_SIZE = 128
EPOCHS = 5
IMG_HEIGHT = 100
IMG_WIDTH = 100

train_image_generator = ImageDataGenerator(rescale=1./255)
valid_image_generator = ImageDataGenerator(rescale=1./255)

train_data_gen = train_image_generator.flow_from_directory(batch_size = BATCH_SIZE,
                                                           directory = train_dir,
                                                           shuffle = True,
                                                           target_size = (IMG_HEIGHT, IMG_WIDTH),
                                                           class_mode = 'binary')

val_data_gen = valid_image_generator.flow_from_directory(batch_size = BATCH_SIZE,
                                                         directory = valid_dir,
                                                         target_size = (IMG_HEIGHT, IMG_WIDTH),
                                                         class_mode = 'binary')

print(val_data_gen.class_indices)

#### MODEL ####
feature_extractor = tf.keras.applications.MobileNetV2(include_top = False, input_shape = (IMG_HEIGHT, IMG_WIDTH, 3))
feature_extractor.trainable = False

model = Sequential([
                    feature_extractor,
                    GlobalAveragePooling2D(),
                    Dense(512, activation = 'relu'),
                    Dropout(0.5),
                    Dense(1, activation = 'sigmoid')
])

model.compile(optimizer = 'adam',
              loss = tf.keras.losses.BinaryCrossentropy(),
              metrics = ['accuracy'])

# !rm -r models # to remove pre-exist models

checkpoint = tf.keras.callbacks.ModelCheckpoint(
    MODEL_PATH, monitor = 'val_loss', verbose = 0, save_best_only = True,
    save_weights_only = False, mode = 'auto', save_freq = 'epoch'
)

history = model.fit(
    train_data_gen,
    steps_per_epoch = total_train // BATCH_SIZE,
    epochs = EPOCHS,
    validation_data = val_data_gen,
    validation_steps = total_valid // BATCH_SIZE,
    callbacks = checkpoint
)

# model compression and download
# from google.colab import drive
# drive.mount('/content/drive')
# !tar cvf models.tar.gz models

# for test data
!git clone https://github.com/narfian/Mask_Test

# !wget http://dlib.net/files/mmod_human_face_detector.dat.bz2 -O mmod_human_face_detector.dat.bz2
# !bzip2 -kd mmod_human_face_detector.dat.bz2
# cnn_face_detector = dlib.cnn_face_detection_model_v1('mmod_human_face_detector.dat')
classifier = tf.keras.models.load_model('models')

#### SHOW ORIGINAL VIDEO ####
TEST_PATH = 'Mask_Test/data'
video_handle = 'test_mask3_people_frontal.mp4'
path_video = os.path.join(TEST_PATH, video_handle)

from IPython.display import HTML
from base64 import b64encode
mp4 = open(path_video, 'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML("""
<video width=800 controls>
  <source src="%s" type="video/mp4">
</video>
""" % data_url)

#### VIDEO MASK DETECTOR ####
cap = cv2.VideoCapture(path_video) # video capture object
ret, frame = cap.read() # read one frame each. if frame is correctly read, ret = True, else ret = False
fourcc = cv2.VideoWriter_fourcc(*'MP4V') # codec information
out = cv2.VideoWriter('./output.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (frame.shape[1],frame.shape[0])) # video writer object to save video. ouput file, codec info, frame per sec, size

while(cap.isOpened()):
    ret, frame = cap.read()
    if ret == False:
      break

    dets = cnn_face_detector(frame, 1)
    for i, d in enumerate(dets):
      BD = 10
      x = d.rect.left() - BD
      y = d.rect.top() - BD
      h = d.rect.bottom() - d.rect.top() + 2*BD
      w = d.rect.right() - d.rect.left() + 2*BD
  
      infimg = copy.deepcopy(frame[y:y+h, x:x+w])
      infimg = cv2.resize(infimg, dsize=(100, 100), interpolation=cv2.INTER_AREA)
      infimg = cv2.cvtColor(infimg, cv2.COLOR_BGR2RGB)
      infimg = infimg/255.
      predicted = classifier.predict(infimg[np.newaxis, ...])

      label = 'Mask' if 0.5 > predicted else 'No Mask'

      cv2.rectangle(frame, (d.rect.left(), d.rect.top()), (d.rect.right(), d.rect.bottom()), (0, 0, 255), 3)

      FONT_FACTOR = frame.shape[1] // 250
      FONT_COLOR = (0, 255, 0) if 0.5 > predicted else (255, 0, 0)
      cv2.putText(frame, label, (d.rect.left(), d.rect.top()), cv2.FONT_HERSHEY_PLAIN, FONT_FACTOR, FONT_COLOR, FONT_FACTOR)
    
    out.write(frame)
    if cv2.waitKey(1) & 0xFF == ord('q'): # if press 'q' key, end
        break

cap.release() # release opened capture object
out.release()

save_path = "output.mp4"

if not os.path.exists("output_compressed"):
  os.makedirs("output_compressed")
compressed_path = os.path.join("output_compressed", os.path.split(save_path)[-1])
print(compressed_path)
os.system(f"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}") # compress video for web play

path_video = compressed_path

#### SHOW MASK DETECTED VIDEO ####
mp4 = open(path_video,'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML("""
<video width=800 controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)
