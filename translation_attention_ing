import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import matplotlib.ticker as ticker

import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from konlpy.tag import Komoran
from attention_codes import * # preprocess
import numpy as np
import konlpy
import unicodedata
import re
import os
import io
import time
import pandas as pd

%config InlineBackend.figure_format = 'retina'

!pip install konlpy
!pip install jpype1==0.7.0
!apt -qq -y install fonts-nanum
 
fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'
font = fm.FontProperties(fname=fontpath, size=9)
plt.rc('font', family='NanumBarunGothic') 
mpl.font_manager._rebuild()

plt.figure(figsize=(5,5))
plt.plot([0,1],[0,1],label="한글 테스트용")
plt.legend()
plt.show()

# dataset from AI HUB
path_to_file = "/content/2_대화체_200226.xlsx"
data = pd.read_excel(path_to_file)

# tokenizer
komoran = Komoran()
def kor_tokenizer(sentence):
  return " ".join(komoran.morphs(sentence))

# Korean - morpheme, English - space
# add <start> <end> tokens to the sentence
SOURCE, TARGET = create_dataset(path_to_file, kor_tokenizer)

# tokenizing
input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(SOURCE, TARGET)
# _tensor: padded index sequence, _lang: tokenizer, vocab list
max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]

# split dataset
input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1, random_state=2021)
# train_test_split: from sklearn.model_selection import

# model
BUFFER_SIZE = len(input_tensor_train)
BATCH_SIZE = 32
steps_per_epoch = len(input_tensor_train)//BATCH_SIZE
vocab_inp_size = len(inp_lang.word_index)+1 # for [PAD]
vocab_tar_size = len(targ_lang.word_index)+1

dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)
# from_tensor_slices: create a dataset whose elements are slices of the given tensors.
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)

embedding_dim = 256
units = 512

# encoder
encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)
# GRU ouput is used when calculate attention (encoder hidden states)
# GRU state is fed as first hidden when decoded
# This class includes embedding and gru layers

# attention layer
attention_layer = BahdanauAttention(10) 
attention_result, attention_weights = attention_layer(sample_hidden, sample_output) 

# decoder
decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)
sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), # token to be processed at this step
                                      sample_hidden, # hidden vector before decoder
                                      sample_output) # hidden vector whch brings information through attention

# compile
optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0)) # padding -> no back-propagation
  loss_ = loss_object(real, pred)
  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask
  return tf.reduce_mean(loss_)

checkpoint_dir = "/content/gdrive/My Drive/NLP/train_attention"
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)


# train
@tf.function
def train_step(inp, targ, enc_hidden):
  loss = 0
  with tf.GradientTape() as tape:
    enc_output, enc_hidden = encoder(inp, enc_hidden)
    dec_hidden = enc_hidden
    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)

    # Teacher forcing algorithm
    for t in range(1, targ.shape[1]):
      # passing enc_output to the decoder
      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)
      loss += loss_function(targ[:, t], predictions)
      dec_input = tf.expand_dims(targ[:, t], 1)

  batch_loss = (loss / int(targ.shape[1]))
  variables = encoder.trainable_variables + decoder.trainable_variables
  gradients = tape.gradient(loss, variables)
  optimizer.apply_gradients(zip(gradients, variables))
  return batch_loss


EPOCHS = 10
for epoch in range(EPOCHS):
  start = time.time()
  enc_hidden = encoder.initialize_hidden_state()
  total_loss = 0
  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
    batch_loss = train_step(inp, targ, enc_hidden)
    total_loss += batch_loss
  if (epoch + 1) % 2 == 0:
    checkpoint.save(file_prefix = checkpoint_prefix)

# infer
def translate(sentence):
  result, sentence, attention_plot = inference(sentence, encoder, decoder, units, max_length_targ, max_length_inp, kor_tokenizer, inp_lang, targ_lang)
  print('Input: %s' % (sentence))
  print('Predicted translation: {}'.format(result))
  attention_plot = attention_plot[:len(result.split()), :len(sentence.split())]
  plot_attention(attention_plot, sentence.split(), result.split())
  
  
## need to study more about encoder, decoder, and attention
